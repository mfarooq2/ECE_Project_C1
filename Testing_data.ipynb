{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eab0847-e9c3-4cfa-a4ca-8f160c770a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "def data_frame_adder(temp_frame,file_name):\n",
    "    file_name=file_name.split('.csv')[0]\n",
    "    temp_frame['file_name']=file_name\n",
    "    temp_frame['subject']=temp_frame['file_name'].apply(lambda x: int(x.split('subject_')[1].split('_')[0]))\n",
    "    temp_frame['session']=temp_frame['file_name'].apply(lambda x: int(x.split('subject_')[1].split('_')[1]))\n",
    "    #temp_frame['type']=temp_frame['file_name'].apply(lambda x: x.split('__')[1])\n",
    "    temp_frame = temp_frame.drop(columns=['file_name'])\n",
    "    temp_frame = temp_frame[temp_frame.columns[::-1]]\n",
    "    temp_frame = temp_frame.sort_values(by=['session','subject'], axis=0)\n",
    "    col_list1 = list(temp_frame.columns)[0:2]\n",
    "    col_list2 = list(temp_frame.columns)[2:]\n",
    "    col_list2.sort()\n",
    "    col_list=col_list1+col_list2\n",
    "    \n",
    "    #temp_frame = swap_columns(temp_frame, 'subject', 'session')\n",
    "    # Swapping the columns\n",
    "    temp_frame = temp_frame.reindex(columns=col_list)\n",
    "\n",
    "    return temp_frame\n",
    "Training_data_files=glob.glob('TestData/*.csv')\n",
    "file_name_list_x=[]\n",
    "file_name_list_x_time=[]\n",
    "file_name_list_y=[]\n",
    "file_name_list_y_time=[]\n",
    "for file_name in tqdm(Training_data_files):\n",
    "    if 'x.csv' in file_name:\n",
    "        file_name_list_x.append(file_name)\n",
    "    elif 'x_time.csv' in file_name:\n",
    "        file_name_list_x_time.append(file_name)\n",
    "    elif 'y.csv' in file_name:\n",
    "        file_name_list_y.append(file_name)\n",
    "    else:\n",
    "        file_name_list_y_time.append(file_name)\n",
    "file_name_list_x.sort()\n",
    "file_name_list_x_time.sort()\n",
    "file_name_list_y.sort()\n",
    "file_name_list_y_time.sort()\n",
    "Subject_list= [f.split('_')[1] for f in file_name_list_x]\n",
    "Session_list= [f.split('_')[2] for f in file_name_list_x]\n",
    "\n",
    "x_sensor_dat=pd.DataFrame([])\n",
    "y_sensor_dat=pd.DataFrame([])\n",
    "x_time_dat=[]\n",
    "for sub,sess in tqdm(zip(Subject_list,Session_list)):\n",
    "    #x_sensor_dat=pd.DataFrame(x_sensor_dat.append(pd.read_csv(f\"TrainingData/subject_{sub}_{sess}__x.csv\",header=None)))\n",
    "    \n",
    "    x_sensor_dat=pd.concat([pd.read_csv(f\"TestData/subject_{sub}_{sess}__x.csv\",header=None),pd.read_csv(f\"TestData/subject_{sub}_{sess}__x_time.csv\",header=None).rename(columns={0:'time_stamp'})],axis=1)\n",
    "    features_to_normalize = [0,1,2,3,4,5]\n",
    "    features_to_normalize = [0,1,2,3,4,5]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_sensor_dat[features_to_normalize])\n",
    "    scaled = scaler.fit_transform(x_sensor_dat[features_to_normalize])\n",
    "    scaled_df = pd.DataFrame(scaled, columns=x_sensor_dat[features_to_normalize].columns)\n",
    "    x_sensor_dat[features_to_normalize]=scaled_df\n",
    "\n",
    "#    x_sensor_dat[features_to_normalize] = x_sensor_dat[features_to_normalize].apply(lambda x:(x-x.mean())/ x.std(), axis=0)\n",
    "    #apply(lambda x:(x-x.min())/(x.max()-x.min()))\n",
    "    x_sensor_dat['measurements']=x_sensor_dat.apply(lambda x: [x[0],x[1],x[2],x[3],x[4],x[5]],axis=1)\n",
    "    x_sensor_dat=x_sensor_dat[['time_stamp','measurements']]\n",
    "    #x_sensor_dat=x_sensor_dat.set_index('time_stamp').asfreq('0.025S')\n",
    "    x_sensor_dat['Subject']=int(sub)\n",
    "    x_sensor_dat['Session']=int(sess)\n",
    "    #x_sensor_dat=x_sensor_dat.set_index('time_stamp').asfreq('0.025S')\n",
    "    \n",
    "    y_sensor_dat=pd.concat([pd.read_csv(f\"TestData/subject_{sub}_{sess}__y.csv\",header=None),pd.read_csv(f\"TestData/subject_{sub}_{sess}__y_time.csv\",header=None).rename(columns={0:'time_stamp'})],axis=1).rename(columns={0:'labels'})\n",
    "    #y_sensor_dat['labels']=y_sensor_dat.apply(lambda y: [y[0],y[1],y[2],y[3]],axis=1)\n",
    "    y_sensor_dat=y_sensor_dat[['time_stamp','labels']]\n",
    "    #y_sensor_dat[\"time_stamp\"]=pd.to_datetime(y_sensor_dat[\"time_stamp\"],unit='s').round('ms')\n",
    "    trp=pd.concat(\n",
    "        [pd.DataFrame({'labels':0},index=[x_sensor_dat.index[-1]]),pd.DataFrame({'time_stamp':np.max(x_sensor_dat['time_stamp'])},index=[x_sensor_dat.index[-1]])\n",
    "                         ]\n",
    "         ,axis=1)\n",
    "    y_sensor_dat=pd.concat([y_sensor_dat,trp],axis=0)\n",
    "    trp=pd.concat([pd.DataFrame({'labels':0},index=[x_sensor_dat.index[0]]),pd.DataFrame({'time_stamp':np.min(x_sensor_dat['time_stamp'])},index=[x_sensor_dat.index[0]])],axis=1)\n",
    "    y_sensor_dat=pd.concat([trp,y_sensor_dat],axis=0)\n",
    "    # y_sensor_dat=y_sensor_dat.set_index('time_stamp').asfreq('0.025S')\n",
    "    #y_sensor_dat['Subject']=int(sub)\n",
    "    #y_sensor_dat['Session']=int(sess)\n",
    "    x_sensor_dat[\"time_stamp\"]=pd.to_datetime(x_sensor_dat[\"time_stamp\"],unit='s').round('ms')\n",
    "    x_sensor_dat=x_sensor_dat.set_index('time_stamp')\n",
    "    x_sensor_dat=x_sensor_dat.asfreq('0.025S')\n",
    "    \n",
    "    y_sensor_dat[\"time_stamp\"]=pd.to_datetime(y_sensor_dat[\"time_stamp\"],unit='s').round('ms')\n",
    "    y_sensor_dat=y_sensor_dat.set_index('time_stamp')\n",
    "    y_sensor_dat=y_sensor_dat.asfreq(freq='0.025S', method='bfill')\n",
    "    x_y_dat=pd.merge(x_sensor_dat,y_sensor_dat, how='inner', left_index=True, right_index=True)\n",
    "    x_y_dat.to_parquet(f\"pre_processed_testing/subject_{sub}_session_{sess}.gzip\",compression='gzip')\n",
    "files=glob.glob('pre_processed_testing/*.gzip')\n",
    "total_training_data=pd.DataFrame([])\n",
    "for f in tqdm(files):\n",
    "    total_training_data=pd.concat([total_training_data,pd.read_parquet(f, engine='auto')],axis=0)\n",
    "total_training_data=total_training_data.sort_values(['Subject','Session','time_stamp'])\n",
    "total_training_data=total_training_data.reset_index()\n",
    "total_training_data['epoch'] = total_training_data['time_stamp'].sub(pd.Timestamp('1970-01-01 00:00:00.000'))\n",
    "total_training_data['epoch']=total_training_data['epoch'].dt.total_seconds()\n",
    "total_training_data=total_training_data[['Subject','Session','epoch','measurements','labels']]\n",
    "total_training_data_grouped=total_training_data.groupby(['Subject','Session','epoch']).agg({'measurements':[np.mean],'labels':[np.mean]})\n",
    "total_training_data_grouped.columns = total_training_data_grouped.columns.droplevel(1)\n",
    "total_training_data_grouped=total_training_data_grouped.reset_index(level=['epoch'])\n",
    "total_training_data_grouped.to_parquet(f\"combined_sampled/testing.gzip\",compression='gzip')\n",
    "testing_combined=pd.read_parquet('combined_sampled/testing.gzip', engine='auto')\n",
    "testing_combined=testing_combined.reset_index()\n",
    "testing_combined=testing_combined[['measurements','labels']]\n",
    "X_test=np.vstack(testing_combined['measurements']).reshape(-1,3,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db91a7ce-df31-41ca-98ac-d35df6cf6c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c97fc3d-b69e-4700-ba76-197e03d5eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 262144.00it/s]\n",
      "4it [00:02,  1.88it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a5e69d-91a5-48b1-80fb-6763a7f334fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 37.23it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865e6c00-5aee-490e-8f0a-f6366d1f010b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a23d57de-f504-4c37-851a-1b1c5d3e9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "323e3e58-479d-4484-88d2-dbcf73847af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 22:56:06.167046: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-17 22:56:06.167458: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def make_model(input_shape,num_classes=num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv2D(filters=64, kernel_size=2, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling2D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"relu\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=X_test.shape[1:],num_classes=num_classes)\n",
    "# epochs = 500\n",
    "# batch_size = 32\n",
    "\n",
    "# callbacks = [\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "#     ),\n",
    "#     keras.callbacks.ReduceLROnPlateau(\n",
    "#         monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "#     ),\n",
    "#     keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "# ]\n",
    "# model.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=\"categorical_crossentropy\",\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e5b540-2c81-4b91-8fa6-8c18a742138e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measurements</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.03765202030266226, 0.09368572626400568, -1...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.03524098598251466, 0.09306726091291935, -1...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.03380178732749251, 0.0932526808924517, -1....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.031089793317064792, 0.0952962881937525, -1...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.025079759587105822, 0.09672302515454348, -...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184147</th>\n",
       "      <td>[0.159406356135594, 0.16277825502985702, -0.47...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184148</th>\n",
       "      <td>[0.16483386876811393, 0.1645012814504027, -0.4...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184149</th>\n",
       "      <td>[0.17994221577331065, 0.15910908643511365, -0....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184150</th>\n",
       "      <td>[0.17649126596710446, 0.16459274505127874, -0....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184151</th>\n",
       "      <td>[0.17248348504854094, 0.16095652798951093, -0....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184152 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             measurements  labels\n",
       "0       [-0.03765202030266226, 0.09368572626400568, -1...     0.0\n",
       "1       [-0.03524098598251466, 0.09306726091291935, -1...     NaN\n",
       "2       [-0.03380178732749251, 0.0932526808924517, -1....     NaN\n",
       "3       [-0.031089793317064792, 0.0952962881937525, -1...     NaN\n",
       "4       [-0.025079759587105822, 0.09672302515454348, -...     NaN\n",
       "...                                                   ...     ...\n",
       "184147  [0.159406356135594, 0.16277825502985702, -0.47...     NaN\n",
       "184148  [0.16483386876811393, 0.1645012814504027, -0.4...     NaN\n",
       "184149  [0.17994221577331065, 0.15910908643511365, -0....     NaN\n",
       "184150  [0.17649126596710446, 0.16459274505127874, -0....     0.0\n",
       "184151  [0.17248348504854094, 0.16095652798951093, -0....     0.0\n",
       "\n",
       "[184152 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c996ef-6c70-415d-bbe1-a69d146a5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
