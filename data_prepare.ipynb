{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1abfd1c-f4df-4e61-bff2-01526bb7d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61568a-0bf8-4f2d-99a1-cc338f21f421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0eb015d-2d7b-499c-8eb7-8560a08a64aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 1304394.81it/s]\n",
      "29it [00:15,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_frame_adder(temp_frame,file_name):\n",
    "    file_name=file_name.split('.csv')[0]\n",
    "    temp_frame['file_name']=file_name\n",
    "    temp_frame['subject']=temp_frame['file_name'].apply(lambda x: int(x.split('subject_')[1].split('_')[0]))\n",
    "    temp_frame['session']=temp_frame['file_name'].apply(lambda x: int(x.split('subject_')[1].split('_')[1]))\n",
    "    #temp_frame['type']=temp_frame['file_name'].apply(lambda x: x.split('__')[1])\n",
    "    temp_frame = temp_frame.drop(columns=['file_name'])\n",
    "    temp_frame = temp_frame[temp_frame.columns[::-1]]\n",
    "    temp_frame = temp_frame.sort_values(by=['session','subject'], axis=0)\n",
    "    col_list1 = list(temp_frame.columns)[0:2]\n",
    "    col_list2 = list(temp_frame.columns)[2:]\n",
    "    col_list2.sort()\n",
    "    col_list=col_list1+col_list2\n",
    "    \n",
    "    #temp_frame = swap_columns(temp_frame, 'subject', 'session')\n",
    "    # Swapping the columns\n",
    "    temp_frame = temp_frame.reindex(columns=col_list)\n",
    "\n",
    "    return temp_frame\n",
    "Training_data_files=glob.glob('TrainingData/*.csv')\n",
    "file_name_list_x=[]\n",
    "file_name_list_x_time=[]\n",
    "file_name_list_y=[]\n",
    "file_name_list_y_time=[]\n",
    "for file_name in tqdm(Training_data_files):\n",
    "    if 'x.csv' in file_name:\n",
    "        file_name_list_x.append(file_name)\n",
    "    elif 'x_time.csv' in file_name:\n",
    "        file_name_list_x_time.append(file_name)\n",
    "    elif 'y.csv' in file_name:\n",
    "        file_name_list_y.append(file_name)\n",
    "    else:\n",
    "        file_name_list_y_time.append(file_name)\n",
    "file_name_list_x.sort()\n",
    "file_name_list_x_time.sort()\n",
    "file_name_list_y.sort()\n",
    "file_name_list_y_time.sort()\n",
    "Subject_list= [f.split('_')[1] for f in file_name_list_x]\n",
    "Session_list= [f.split('_')[2] for f in file_name_list_x]\n",
    "\n",
    "x_sensor_dat=pd.DataFrame([])\n",
    "y_sensor_dat=pd.DataFrame([])\n",
    "x_time_dat=[]\n",
    "for sub,sess in tqdm(zip(Subject_list,Session_list)):\n",
    "    #x_sensor_dat=pd.DataFrame(x_sensor_dat.append(pd.read_csv(f\"TrainingData/subject_{sub}_{sess}__x.csv\",header=None)))\n",
    "    \n",
    "    x_sensor_dat=pd.concat([pd.read_csv(f\"TrainingData/subject_{sub}_{sess}__x.csv\",header=None),pd.read_csv(f\"TrainingData/subject_{sub}_{sess}__x_time.csv\",header=None).rename(columns={0:'time_stamp'})],axis=1)\n",
    "    features_to_normalize = [0,1,2,3,4,5]\n",
    "    features_to_normalize = [0,1,2,3,4,5]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_sensor_dat[features_to_normalize])\n",
    "    scaled = scaler.fit_transform(x_sensor_dat[features_to_normalize])\n",
    "    scaled_df = pd.DataFrame(scaled, columns=x_sensor_dat[features_to_normalize].columns)\n",
    "    x_sensor_dat[features_to_normalize]=scaled_df\n",
    "\n",
    "#    x_sensor_dat[features_to_normalize] = x_sensor_dat[features_to_normalize].apply(lambda x:(x-x.mean())/ x.std(), axis=0)\n",
    "    #apply(lambda x:(x-x.min())/(x.max()-x.min()))\n",
    "    x_sensor_dat['measurements']=x_sensor_dat.apply(lambda x: [x[0],x[1],x[2],x[3],x[4],x[5]],axis=1)\n",
    "    x_sensor_dat=x_sensor_dat[['time_stamp','measurements']]\n",
    "    #x_sensor_dat=x_sensor_dat.set_index('time_stamp').asfreq('0.025S')\n",
    "    x_sensor_dat['Subject']=int(sub)\n",
    "    x_sensor_dat['Session']=int(sess)\n",
    "    #x_sensor_dat=x_sensor_dat.set_index('time_stamp').asfreq('0.025S')\n",
    "    \n",
    "    y_sensor_dat=pd.concat([pd.read_csv(f\"TrainingData/subject_{sub}_{sess}__y.csv\",header=None),pd.read_csv(f\"TrainingData/subject_{sub}_{sess}__y_time.csv\",header=None).rename(columns={0:'time_stamp'})],axis=1).rename(columns={0:'labels'})\n",
    "    #y_sensor_dat['labels']=y_sensor_dat.apply(lambda y: [y[0],y[1],y[2],y[3]],axis=1)\n",
    "    y_sensor_dat=y_sensor_dat[['time_stamp','labels']]\n",
    "    #y_sensor_dat[\"time_stamp\"]=pd.to_datetime(y_sensor_dat[\"time_stamp\"],unit='s').round('ms')\n",
    "    trp=pd.concat(\n",
    "        [pd.DataFrame({'labels':0},index=[x_sensor_dat.index[-1]]),pd.DataFrame({'time_stamp':np.max(x_sensor_dat['time_stamp'])},index=[x_sensor_dat.index[-1]])\n",
    "                         ]\n",
    "         ,axis=1)\n",
    "    y_sensor_dat=pd.concat([y_sensor_dat,trp],axis=0)\n",
    "    trp=pd.concat([pd.DataFrame({'labels':0},index=[x_sensor_dat.index[0]]),pd.DataFrame({'time_stamp':np.min(x_sensor_dat['time_stamp'])},index=[x_sensor_dat.index[0]])],axis=1)\n",
    "    y_sensor_dat=pd.concat([trp,y_sensor_dat],axis=0)\n",
    "    # y_sensor_dat=y_sensor_dat.set_index('time_stamp').asfreq('0.025S')\n",
    "    #y_sensor_dat['Subject']=int(sub)\n",
    "    #y_sensor_dat['Session']=int(sess)\n",
    "    x_sensor_dat[\"time_stamp\"]=pd.to_datetime(x_sensor_dat[\"time_stamp\"],unit='s').round('ms')\n",
    "    x_sensor_dat=x_sensor_dat.set_index('time_stamp')\n",
    "    x_sensor_dat=x_sensor_dat.asfreq('0.025S')\n",
    "    \n",
    "    y_sensor_dat[\"time_stamp\"]=pd.to_datetime(y_sensor_dat[\"time_stamp\"],unit='s').round('ms')\n",
    "    y_sensor_dat=y_sensor_dat.set_index('time_stamp')\n",
    "    y_sensor_dat=y_sensor_dat.asfreq(freq='0.025S', method='bfill')\n",
    "    x_y_dat=pd.merge(x_sensor_dat,y_sensor_dat, how='inner', left_index=True, right_index=True)\n",
    "    x_y_dat.to_parquet(f\"pre_processed_training/subject_{sub}_session_{sess}.gzip\",compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d91b20-3d6a-43ec-b119-815084b2ed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 31.38it/s]\n"
     ]
    }
   ],
   "source": [
    "files=glob.glob('pre_processed_training/*.gzip')\n",
    "total_training_data=pd.DataFrame([])\n",
    "for f in tqdm(files):\n",
    "    total_training_data=pd.concat([total_training_data,pd.read_parquet(f, engine='auto')],axis=0)\n",
    "total_training_data=total_training_data.sort_values(['Subject','Session','time_stamp'])\n",
    "total_training_data=total_training_data.reset_index()\n",
    "total_training_data['epoch'] = total_training_data['time_stamp'].sub(pd.Timestamp('1970-01-01 00:00:00.000'))\n",
    "total_training_data['epoch']=total_training_data['epoch'].dt.total_seconds()\n",
    "total_training_data=total_training_data[['Subject','Session','epoch','measurements','labels']]\n",
    "total_training_data_grouped=total_training_data.groupby(['Subject','Session','epoch']).agg({'measurements':[np.mean],'labels':[np.mean]})\n",
    "total_training_data_grouped.columns = total_training_data_grouped.columns.droplevel(1)\n",
    "total_training_data_grouped=total_training_data_grouped.reset_index(level=['epoch'])\n",
    "total_training_data_grouped.to_parquet(f\"combined_sampled/training.gzip\",compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c2634-58fa-4a13-86ba-9d02848b81df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
